{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4fe56e",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbour On MNIST Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86068d62",
   "metadata": {},
   "source": [
    "The objective of this dataset is to accurately classify handwritten digits from 0 to 9. Instead of using the full MNIST dataset, which contains 60,000 training images and 10,000 testing images, we will work with a smaller subset provided by the scikit-learn library. This subset includes 1,797 digit images, which we will divide into training, validation, and testing sets.\n",
    "Each image is originally an 8 x 8 grayscale image, but scikit-learn converts it into a flattened list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a863aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from __future__ import print_function\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import imutils\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41e33907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Mnist data digits \n",
    "mnist = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67610881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the MNIST data and construct the training and testing split, using 75% of the\n",
    "# data for training and 25% for testing\n",
    "(trainData, testData, trainLabels, testLabels) = train_test_split(np.array(mnist.data),\n",
    "\tmnist.target, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4d96e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, let's take 10% of the training data and use that for validation\n",
    "(trainData, valData, trainLabels, valLabels) = train_test_split(trainData, trainLabels,\n",
    "\ttest_size=0.1, random_state=84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9412987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data points: 1212\n",
      "validation data points: 135\n",
      "testing data points: 450\n"
     ]
    }
   ],
   "source": [
    "# show the sizes of each data split\n",
    "print(\"training data points: {}\".format(len(trainLabels)))\n",
    "print(\"validation data points: {}\".format(len(valLabels)))\n",
    "print(\"testing data points: {}\".format(len(testLabels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552569c5",
   "metadata": {},
   "source": [
    "Now that we have our data splits taken care of, let’s train our classifier and find the optimal value of k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4d622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the values of k for our k-Nearest Neighbor classifier along with the\n",
    "# list of accuracies for each value of k\n",
    "kVals = range(1, 30, 2)\n",
    "accuracies = []\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "292fe7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1, accuracy=99.26%\n",
      "k=3, accuracy=99.26%\n",
      "k=5, accuracy=99.26%\n",
      "k=7, accuracy=99.26%\n",
      "k=9, accuracy=99.26%\n",
      "k=11, accuracy=99.26%\n",
      "k=13, accuracy=99.26%\n",
      "k=15, accuracy=99.26%\n",
      "k=17, accuracy=98.52%\n",
      "k=19, accuracy=98.52%\n",
      "k=21, accuracy=97.78%\n",
      "k=23, accuracy=97.04%\n",
      "k=25, accuracy=97.78%\n",
      "k=27, accuracy=97.04%\n",
      "k=29, accuracy=97.04%\n"
     ]
    }
   ],
   "source": [
    "# loop over various values of `k` for the k-Nearest Neighbor classifier\n",
    "for k in range(1, 30, 2):\n",
    "# train the k-Nearest Neighbor classifier with the current value of `k`\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(trainData, trainLabels)\n",
    "#After our model is trained, we need to evaluate it using our validation data    \n",
    "# evaluate the model and update the accuracies list\n",
    "    score = model.score(valData, valLabels)\n",
    "    print(\"k=%d, accuracy=%.2f%%\" % (k, score * 100))\n",
    "    accuracies.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f974ff7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=1 achieved highest accuracy of 99.26% on validation data\n"
     ]
    }
   ],
   "source": [
    "# find the value of k that has the largest accuracy\n",
    "i = int(np.argmax(accuracies))\n",
    "print(\"k=%d achieved highest accuracy of %.2f%% on validation data\" % (kVals[i],\n",
    "accuracies[i] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390b94b1",
   "metadata": {},
   "source": [
    "Although the accuracy for \\( k = 1 \\) through \\( k = 15 \\) remained consistent, using just one neighbor significantly improves efficiency. Therefore, we will use \\( k = 1 \\) for training and evaluating our classifier on the final test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89d9c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-train our classifier using the best k value and predict the labels of the\n",
    "# test data\n",
    "model = KNeighborsClassifier(n_neighbors=kVals[i])\n",
    "model.fit(trainData, trainLabels)\n",
    "predictions = model.predict(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6d30715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION ON TESTING DATA\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        43\n",
      "           1       0.95      1.00      0.97        37\n",
      "           2       1.00      1.00      1.00        38\n",
      "           3       0.98      0.98      0.98        46\n",
      "           4       0.98      0.98      0.98        55\n",
      "           5       0.98      1.00      0.99        59\n",
      "           6       1.00      1.00      1.00        45\n",
      "           7       1.00      0.98      0.99        41\n",
      "           8       0.97      0.95      0.96        38\n",
      "           9       0.96      0.94      0.95        48\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.98      0.98      0.98       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show a final classification report demonstrating the accuracy of the classifier\n",
    "# for each of the digits\n",
    "print(\"EVALUATION ON TESTING DATA\")\n",
    "print(classification_report(testLabels, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464f16a4",
   "metadata": {},
   "source": [
    "Achieving 98% accuracy is impressive! Additionally, digits 0, 2, 6, and 7 are classified correctly 100% of the time. The digit 1 has the lowest classification accuracy, at 95%.\n",
    "Achieving high accuracy on the MNIST dataset doesn't mean handwritten digit recognition is \"solved.\" Despite MNIST being a standard benchmark, its images are heavily pre-processed—cropped, thresholded, and centered—which doesn’t reflect real-world conditions. In practice, real-world datasets are often less clean and require feature extraction beyond raw pixel intensities. Nonetheless, this exercise demonstrates how Euclidean distance can yield high accuracy with well-pre-processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76718efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loop over a few random digits\n",
    "for i in np.random.randint(0, high=len(testLabels), size=5):\n",
    "    # Grab the image and classify it\n",
    "    image = testData[i]\n",
    "    prediction = model.predict(image.reshape(1, -1))[0]\n",
    "\n",
    "    # Convert the image for an 64-dim array to an 8 x 8 image compatible with OpenCV,\n",
    "    # then resize it to 32 x 32 pixels so we can see it better\n",
    "    image = image.reshape((8, 8)).astype(\"uint8\")\n",
    "    image = exposure.rescale_intensity(image, out_range=(0, 255))\n",
    "\n",
    "    # Resize the image to 32 x 32 pixels\n",
    "    image = imutils.resize(image, width=32, inter=cv2.INTER_CUBIC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ceb92",
   "metadata": {},
   "source": [
    "Let’s conclude this code example by reviewing some individual predictions made by our k-NN classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80176289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Show the prediction\n",
    "print(\"I think that digit is: {}\".format(prediction))\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow(\"Image\", image)\n",
    "\n",
    "# Wait indefinitely until a key is pressed\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d492bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
